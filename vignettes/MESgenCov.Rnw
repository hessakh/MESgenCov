%\VignetteEngine{knitr::knitr}
% !Rnw weave = knitr
%\VignetteIndexEntry{MESgenCov: An R Package for ...}

\documentclass[11pt]{article}
\usepackage[left=1.1cm, top=2.54cm, right=1.1cm, bottom=2.54cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{cite}
\usepackage{colortbl}
\usepackage{comment}
\usepackage{float}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{subcaption}
\usepackage[table]{xcolor}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage[normalem]{ulem}
\usepackage{wrapfig}

 \geometry{
 a4paper,
 total={170mm,210mm},
 left=20mm,
 top=20mm,
 }


\title{MESgenCov: An R Package for generating covariance matrices \\ from precipitation chemistry data}
\author{Hessa Al-Thani$^1$ \qquad Jon Lee$^2$\\[0.35cm]
  {\small Dept. of Industrial and Operations Engineering, Univ. of Michigan, Ann Arbor, MI 48105 USA }\\[0.35cm]{\small{$^1$hessakh@umich.edu}} \qquad {\small{$^2$jonxlee@umich.edu}}
}
\date{\textbf{MESgenCov} \Sexpr{packageDescription("MESgenCov2")$Version}  }
%

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(MESgenCov)
@
<<include=FALSE>>=

@

\maketitle

\begin{abstract}
  We present an R package for temporally fitting multivariate precipitation chemistry data and extracting a covariance matrix
for use in MES (maximum-entropy sampling).
 We provide multiple functionalities for modeling and model assessment. The package uses data from the NADP/NTN (National Atmospheric Deposition Program / National Trends Network) on their set of more than 370 monitoring sites, 1978--present. The user specifies the sites, chemicals and time period desired, fits a user-defined univariate model for each site and chemical selected, and produces a covariance matrix for use by MES algorithms.

\end{abstract}

\section{The MES problem}

The MES (maximum-entropy sampling) problem (see \cite{ShewryWynn,SebWynn,FedorovLee,LeeEnv}) has been applied to many domains where the objective is to determine a most informative subset $S$, of presepcified size $s$, from a set of $n$ Gaussian variables $N$. This is done by seeking to maximize the (log) determinant of the covariance matrix for some $S \subseteq N$ with $|S|=s$ (see \cite{KLQ,LeeConstrained,AFLW_Using,LeeWilliams_ILP,HLW,AnstreicherLee_Masked,BurerLee,AnstreicherBQPEntropy,linx}). A key area of application has been in environmental montoring (see \cite{Zidek1,Zidek2,Zidek3}, for example).
In this R package, we use  precipitation chemistry data
(ammonium,
calcium,
chloride,
hydrogen,
 magnesium,
 nitrate,
pH,
 potassium,
  sodium, and
sulfate) gathered by the NADP (National Atmospheric Deposition Program) at over 370 sites across the U.S.A. (see \cite{NADPNTN}).
For these instances of the MES problem, $n$ \emph{user-specified} site/chemical pairs comprise $N$.
%%add a bit on generalized getCov after finishing the function

\section{NADP data description}

The NADP maintains the NTN (National Trends Network); this network measures the chemistry of precipitation at monitoring sites across the
U.S.A. Our R package makes use of the weekly data measuring  mg/L of chemicals, such as sulfates, in collected precipitation. We also use the daily data measuring precipitation at each site. Both datasets are available in our package and can be loaded respectively as


<<chunk1, include=TRUE, eval=TRUE>>=
 data("weeklyCSV")
 data("preDailyCSV")
@

A full description of the data can be obtained at \href{http://NADP.slh.wisc.edu/data/ntn/meta/ntn-weekly-Meta.pdf}{NADP/NTN weekly meta}. For a full description of the daily precipitation data, see \href{http://NADP.slh.wisc.edu/data/ntn/meta/ntn-daily-Meta.pdf}{NADP/NTN daily meta}. Small snapshots of the data can also be viewed using:

<<chunk2,include=TRUE,eval=TRUE>>=
  weeklyCSV[1:6,c(1:4,9)]
@

This outputs the first 6 rows, first 4 columns and 9th column of the weekly raw data, columns 5 to 8 contain data that isn't relevant to our analysis. 
\section{MESgenCov implementation}

 The \textbf{MESgenCov} package contains functions in the \texttt{S3} class to create a covariance matrix from the desired subset of NADP data. The function \verb;getCov(); returns a covariance matrix, a list of univariate model summaries, and a table of normality tests produced by the MVN R package. The covariance matrix is produced from a subset of the NADP/NTN data that is specified by the user. For sites with missing data, \verb;getCov(); will fill in predicted values based on the univariate model for each site (see section \ref{sec:3.1}). To avoid sites with a small sample size for the specified time-frame, the function \verb;getSites(); outputs a vector of the sites with the largest sample of data for a given time-frame and measured chemical (see section \ref{sec:3.2}). To find sites that are spatially ``spread out'' but have at least some specified sample size, the function \verb;maxDistSites(); can be used (see section \ref{sec:3.2}).

  \subsection{getCov}
  \label{sec:3.1}
  \verb;getCov(); takes a 15 column dataframe as input where each column corresponds to one of the following user-specifications, given in the table below.The 15 specifications in the input allow the user to specify the subset of data to analyze and gives the user options in displaying different parts of the analysis.

  %%table

  <<message=FALSE, echo=FALSE, warning=FALSE, eval=TRUE>>=
  text_tbl <- data.frame(
  Arguments = c("weeklyB", "startdateStr", "enddateStr", "comp","use36", "siteAdd",  "outlierDatesbySite", "siteOutliers", "removeOutliers", "plotMulti", "sitePlot", "plotAll", "writeMat", "seas", "r", "k"),
  Definition = c( "TRUE if weekly data should be analyzed and FALSE if monthly data should be analyzed", "Date and time of when to start analyzing the data, in the format = “m/d/y H:M”", "Date and time of when to stop analyzing the data, in the format = “m/d/y H:M”", "Vector of strings of pollutants or acidity levels to be analyzed, the pollutants name should be used as it appears in weeklyCSV", "TRUE if default 36 sites should be added, FALSE otherwise", "List of strings of siteIDs that should be analyzed", "List of sites where outliers should be analyzed","List of sites where outliers should be removed" ,"Specify siteID string for outlier analysis",  "TRUE if multivariate analysis plots should be displayed, FALSE otherwise", "Specify siteID to be plotted", "TRUE if plots for all sites should be displayed, FALSE otherwise", "TRUE if  .mat file of the resulting covariance matrix should be written in the working directory", "Approximate periodicity of data, typically 12 for monthly data and 52 for weekly data", "Integer <=5, see univariate model","Integer <= 5, see univariate model"
  )
  )
@

<<message=FALSE, echo=FALSE, warning=FALSE, eval=TRUE>>=
library(magrittr)
library(kableExtra)
    kable(text_tbl, format = "latex", booktabs = T) %>%
    kable_styling(full_width = F) %>%
    column_spec(1, bold = T, color = "black") %>%
    column_spec(2, width = "30em") %>%
        kable_styling(latex_options = "striped")
@
A default set of inputs can be found in the internally stored dataframe "defaultInput". After storing it in a variable in the user's workspace the input can be changed in the following way:
 <<chunckdfInput,include=TRUE,eval=TRUE>>=
  data("defaultInput")
  df <- defaultInput
  df
  df$enddateStr   <- "12/31/88 00:00"
 @

The function \texttt{getCov} produces the following outputs:
\begin{enumerate}

\item Multivariate and univariate normality
<<chunckCovnewInput,include=TRUE,eval=TRUE>>=
  df$plotMulti    <- TRUE
  df$k            <- 3
  g <-getCov(df)
@

\begin{center}
\includegraphics[width = 75mm]{pic1GetCov}
\end{center}

% Remove suggested outliers
% <<chunckCovMultOut,include=TRUE,eval=FALSE>>=
%   getCov(FALSE,"01/01/83 00:00", "12/31/89 00:00",TRUE,NULL,NULL,
%          FALSE,NULL,"SO4",TRUE,TRUE,"OH49",FALSE,FALSE,12,1,3)
% @
%%add figure of code ^^

<<chunckCuni,include=TRUE,eval=FALSE>>=
  g$univariateTest
@
%%

\begin{center}
\includegraphics[width = 75mm]{g2univ}
\end{center}


\item Outlier test for specific tests
<<chunckCovSite,include=TRUE,eval=TRUE>>=
  df$siteOutliers <- list(c("IN41"))
  df$sitePlot     <- "IN41"
  g <- getCov(df)
  i <- match("IN41",g$sites)
  g$rosnerTest[[i]]
@
%%
\begin{center}
\includegraphics[width = 70mm]{rosnerTest}
\end{center}


<<chunckCovSiteOutremoved,include=TRUE,eval=FALSE>>=
  df$outlierDatesbySite <- c("IN41",25)
  getCov(df)
@
\begin{center}
\includegraphics[width = 75mm]{IN41trans}
\end{center}


\item Plot all sites
<<chunckCovPlotALL,include=TRUE,eval=FALSE>>=
  df$plotAll <- TRUE
  getCov(df)
@

\begin{center}
  \includegraphics[width=75mm]{plotAll}
\end{center}

\item Covariance matrix
%%%%Parsing results
<<chunkCovResult,include=TRUE,eval=TRUE>>=
  df$use36        <- FALSE
  df$siteAdd      <- list(c("OH71", "WV18", "MI53"))
  df$siteOutliers <- NULL
  result          <- getCov(df)
  round(result$cov,digits = 4)
@

\item Save covariance matrix as .mat file

This is done by simply setting the 5th last input to TRUE then the .mat file will be saved to the user's current working directory.

<<chunkCovMat,include=TRUE,eval=FALSE>>=
  df$writeMat <- TRUE
  result      <- getCov(df)
@



\item Univariate model summaries
%%%%Parsing results
<<chunkListModResult,include=TRUE,eval=TRUE>>=
  result <- getCov(df)
  sites  <- result$sites
  il = match(c("OH71"),sites)
  result$listMod[il]
@

\item Output all MVN package analysis\\
\noindent
\\
\noindent
The following output is a call to the MVN package that produces mutivariate analysis based on the Mardia method, univariate analysis based on the Shapiro-Wilson method and an multivariate outlier test that is presented as a plot and not as an output in the user's R console.


<<chunkMVNResult,include=TRUE,eval=TRUE>>=
  result <- getCov(df)
  result$mvn
@

The specific call the the MVN package is,

<<chunkMVNcall,include=TRUE,eval=FALSE>>=
 mvn(dfRes[,-1], subset = NULL, mvnTest = "mardia", covariance = TRUE, 
     tol = 1e-25, alpha = 0.5, scale = FALSE, desc = TRUE, transform = "none", 
     univariateTest = "SW",  univariatePlot = "none", multivariatePlot ="none", 
     multivariateOutlierMethod = "none", bc = FALSE, bcType = "rounded", 
     showOutliers = FALSE, showNewData = FALSE)

@

See \cite{MVN} for details on the MVN package.

\item Output dataframe of residuals\\
<<chunkdfRes,include=TRUE,eval=TRUE>>=
  result <- getCov(df)
  result$residualData[1:5,]
@


\end{enumerate}

  \subsection{Functions for getting a vector of sites}
    \label{sec:3.2}
 \texttt{getCov()} takes site lists as input, the function \texttt{getSites()} produces a list of sites with available data for a specified time frame. The code below produces a list of 36 sites with the most weekly data between the years 1983-1986. 

<<getSites1,include=TRUE,eval=TRUE>>=
  result <- getSites("01/01/83 00:00", "12/31/86 00:00",36,104,"SO4","")
  result$finalList
@

The 4th input specifies the minimum sample of weekly data required to be included in the produced list and the last input tells the function to only look at sites in the North Eastern region of the US. Other options for region "W","S","N", see Appendix A for the precise geographic split. 

<<getSites1region,include=TRUE,eval=TRUE>>=
  NSites <- getSites("01/01/83 00:00", "12/31/86 00:00",36,104,"SO4","N")
  NSites$finalList
@

The function \texttt{maxDistSites()} prioritizes sites that are farther away from each other.

<<maxDistSites1,include=TRUE,eval=TRUE>>=
  maxdist <- maxDistSites("01/01/83 00:00", "12/31/86 00:00",36,104,"SO4",1)
  maxdist$finalList
@

  \subsection{Lambert's W transformation on univariate data}
    \label{sec:3.3}
  For a number of sites the resdiuals produced by the deterministic model have skewed distributions with heavy tails. In particular, this is the case for many sites when the sample of data is taken over a period longer than 4 years. To deal with this issue we've incorporated functions from the LambertW package (see \cite{LambertW})in the function \texttt{lambertWtransform()} that will allow a user to transform the residuals produced by the deterministic univariate model. The LambertW package estimates the parameters that fit a LamberW distribution on the given univariate data. Then the underlying gaussian distribution implied by the LambertW distribution is extracted and is used for the multivariate analysis in the function \texttt{lambertWtransform()}. The \texttt{lambertWtransform()} function takes the following as input: a dataframe of residuals, and two binary inputs specifying whether to plot the multivariate qq plot and whether to produce the mat file containing the covariance matrix with the Lambert W transformed residuals. Details on the algorithms that perform the transformation can be found in \cite{LambertW2}.
  
<<lambert1,include=TRUE,eval=TRUE>>=
  data("dfRes50")
  loutput <- lambertWtransform(dfRes = dfRes50, plotMulti = FALSE, writeMat = FALSE)
  loutput$mvn$multivariateNormality
@

 This function will produce a list of four elements that can be called using the following names:
  <<lambert2,include=TRUE,eval=FALSE>>=
  data("dfRes50")
  dfRes50 <- dfRes50
  loutput <- lambertWtransform(dfRes = dfRes50, plotMulti = FALSE, writeMat = FALSE)
  loutput$mvn
  loutput$cov
  loutput$newResiduals
  loutput$univariateTest
@
 
 \begin{enumerate}
   \item \texttt{loutput\$mvn} will show the results of applying the multivariate analysis by the MVN package
   \item \texttt{loutput\$cov} will output the covariance matrix produced by the transformed residuals
   \item \texttt{loutput\$newResiduals} will output the dataframe of Lambert W transformed         residuals
   \item \texttt{loutput\$univariateTest} will output the univariate tests produced by the MVN function for the   transformed residuals
 \end{enumerate}
  
<<chunckIndExample,include=TRUE,eval=FALSE>>=
#get list of sites
maxd  <- maxDistSites("01/01/86 00:00","12/31/94 00:00",50,200,"SO4",1)
#create input dataframe
df <- defaultInput

df$siteAdd      <- list(maxd$finalList)
df$startdateStr <- maxd$startDate
df$use36        <- FALSE
df$comp         <- maxd$comp
df$enddateStr   <- maxd$endDate
df$writeMat     <- TRUE
output          <- getCov(df)
output$mvn$multivariateNormality

loutput <- lambertWtransform(output$residualData, TRUE,FALSE)
loutput$mvn$multivariateNormality
@

<<chunckExampleInd,include=TRUE,eval=TRUE>>=
maxd1  <- maxDistSites("01/01/86 00:00","12/31/94 00:00",50,200,"SO4",1)
df <- defaultInput
df$comp         <- maxd1$comp
df$enddateStr   <- maxd1$endDate
df$startdateStr <- maxd1$startDate
df$siteAdd      <- list(maxd1$finalList)
result          <- getCov(df)
indp            <- independenceTest(result$residualData)
indp$test
@

\section{Methodology}
  \subsection{NADP data processing}
  We process the raw NADP data in a similar way to earlier uses in the context of the MES problem in the field of environmental statistics.
  \cite{Zidek3}
 analyzes the levels of a chemical's concentration by summing weekly quantities (mg) of the chemical, over a month,
 and dividing the monthly total by total precipitation (L), over dates in that month, to get monthly values of sulfate concentration (mg/L).
 For a given monitoring site, chemical, and month $t=0,1,\ldots,T-1$, let
      \begin{align*}
        W(t) &:= \text{set of weeks in month $t$,}\\
        D(w) &:= \text{set of days in week $w$,} \\
        c_w  &:= \text{recorded chemical concentration (mg/L) for week $w$}\\
             &\phantom{:=} \text{ ($c_w=*$ denotes an unrecorded value),}\\
        p_d  &:= \text{recorded precipitation quantity (L) for day $d$,}\\
        p_w  &:= \text{precipitation quantity (L) for week $w$; $\textstyle p_w=\sum_{d\in D(w)}p_d$.}
    \end{align*}
Then the chemical's concentration (mg/L) for month $t$ is calculated as
\[
y(t):=\frac{\sum_{w\in W(t) : c_w \not=*} p_w c_w}
{\sum_{w\in W(t) : c_w \not=*} \sum_{d\in w} p_d}.
\]
It should be noted that when there is no weekly value available for the chemical quantity,
    we do not use the preciptation values for any of the days in such a week
    (so as to not artificially dilute the chemical concentration level for the month).

Finally, a model is fit to $\log(y(t))$.
      In \cite{Zidek3} they use the following model to deseasonalize and detrend the data set of log transformed monthly sulfate concentration values.

\begin{equation} \label{simplemodel}
         \log(y(t))\approx \beta_{1} + \beta_{2}t + \beta_{3}\cos\bigg(\frac{2\pi t}{12}\bigg) + \beta_{4}\sin\bigg(\frac{2\pi t}{12}\bigg)~.
\end{equation}
Basically, this is just an affine model $ \beta_{1} + \beta_{2}t$ plus
    a sinusoidal model with monthly periodicity and intercept $\beta_3$.


  We found that this model did well in normalizing the error for
  certain cites but some sites like "MD13" and "NC03" did not do as well. Rather than
  fix \eqref{simplemodel} as our model, we provide a more flexible model described in the next section.%also show figures for sites

  \subsection{The univariate model}
  %%Confirm that we should not write about what we tries, just what worked
  %%i.e. don't write about ARIMA model
%
%   Through a few experiments we saw that odd powers of trigonometric functions fit the data well but different sites fitted better with different powers of $\sin (\frac{2t\pi}{12})$. In which case we would have had to fit a non-linear regression, to avoid this we use the fact that $\cos^p(t)$ has the following identities:r$
%   \begin{align}
%     \cos^3(t) & = \frac{3}{4}\cos(t) + \frac{1}{4}\cos(3t) \\
%     \cos^5(t) & = \frac{5}{8}\cos(t) + \frac{5}{16}\cos(3t) + \frac{1}{16}\cos(5t)
%   \end{align}
%
% Then we can fit the data at each site using:
%
%   \[\mu(s_i,t) = \beta_{1} + \sum_{i=1}^{r}\beta_{i}t^i + \sum_{j=1}^{k}\beta_{j3}\cos\bigg(\frac{2\pi tj}{S}\bigg) \tag{5}\]
%
% We also added the flexibility of deciding the periodicity of the function, the user can now alter the input $S$ to change the period of the model.
The general model that we provide is
\[
\log(y(t))\approx \sum_{i=0}^r \beta_{i}t^i + \sum_{j=1}^{k} \bigg[a_j \cos \bigg(\frac{2\pi jt}{S}\bigg) +  b_j \sin \bigg(\frac{2\pi jt}{S}\bigg)\bigg].
\]
The user can specify the degree $r$ for the polynomial part of the model which we think of as a truncated taylor series, aimed at capturing aperiodic trends.
Periodic trends are captured via a truncated Fourier series, truncated at level $k$.
The simple model \eqref{simplemodel} is this one with $r=1$, $k=1$ and $S=12$.


\subsection{Internal data sets}

<<chunckExamplemax,include=TRUE,eval=FALSE>>=
#sites with maximum distance data sets, get 50 sites
maxd1  <- maxDistSites("01/01/86 00:00","12/31/94 00:00",50,250,"SO4",1) 
maxd2  <- maxDistSites("01/01/07 00:00","12/31/14 00:00",50,230,"SO4",1) 
maxd3  <- maxDistSites("01/01/07 00:00","12/31/14 00:00",50,230,"NO3",1) 
maxd4  <- maxDistSites("01/01/07 00:00","12/31/14 00:00",50,230,"Na",1)  
maxd5  <- maxDistSites("01/01/07 00:00","12/31/14 00:00",50,230,"NH4",1) 
@

\section*{Acknowledgments} J. Lee was funded by the
Air Force Office of Scientific Research (Complex Networks program), FA9550-19-1-0175.
H. Al-Thani was funded by the Qatar National Research Fund (Graduate Sponsorship Research Program). The authors are very grateful to Dr. Martin Shafer and Robert Larson for helping us gain access to the NADP/NTN data in a convenient form.

\section{Appendix}
Appendix A
\begin{center}
\includegraphics[width = 60mm]{mapDivision}
\end{center}

\bibliographystyle{alpha}
\bibliography{MESgenCov2}

\end{document}


